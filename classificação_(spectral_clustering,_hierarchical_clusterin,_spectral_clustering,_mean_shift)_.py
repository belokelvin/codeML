# -*- coding: utf-8 -*-
"""Classificação (Spectral Clustering, Hierarchical Clusterin, Spectral Clustering, Mean Shift) .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dReA-OVZ0Bpm82PvtsDyVirLw2mXQAK7
"""

!pip install wget
!pip install unidecode

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize
import re
from unidecode import unidecode
import string
import wget

# Baixando o conjunto de dados
'''
url = 'https://raw.githubusercontent.com/belokelvin/ml_testes/main/data_novos.csv'
filename = wget.download(url)
data = pd.read_csv(filename)
'''
data = pd.read_csv('dataset - Página1.csv')
data.to_csv('Dados')
data = data[['Label', 'Disciplina', 'Ementa']]
data

# NLP Classificação e Agrupamento usando TF-IDF + EDA
nltk.download('popular')
nltk.download('stopwords')
palavras_vazias = nltk.corpus.stopwords.words('portuguese')

# Função para pré-processamento de texto
def preprocess_text(text):
    text = text.lower()  # Conversão para minúsculas
    text = unidecode(text)  # Remoção de acentos
    text = re.sub(r"[^a-zA-Z+']", ' ', text)  # Manutenção de caracteres / remoção de números
    text = re.sub(r'\W', ' ', text)  # Remoção de caracteres não-alfanuméricos
    text = re.sub(r'\s+', ' ', text).strip()  # Remoção de espaços extras
    text = re.sub('\n', " . ", text)  # Substituição de quebras de linha por pontos
    return text

# Aplicação da função de pré-processamento à coluna 'Ementa'
data['Ementa'] = data['Ementa'].apply(preprocess_text)
data = data.dropna()  # Remoção de linhas com valores ausentes

# Tokenização das sentenças
sentences = []
for sentence in data['Ementa']:
    sentence = sentence.translate(str.maketrans('', '', string.punctuation))
    sentence = sentence.replace('"', '')
    sentences.append(sentence)

data['Ementa'] = pd.DataFrame(sentences)

data = data.dropna()

# Tokenização das palavras
i = 0
all_tokens = []
for report in data['Ementa']:
    proto = word_tokenize(str(report))
    all_tokens.append(proto)

dftoken = pd.DataFrame({'Tokens': all_tokens})
data['Tokenized'] = all_tokens
data = data.dropna()

!python -m spacy download pt_core_news_sm
!pip install lemminflect

# Remoção de stopwords e junção das palavras
all = []
for list1 in data['Tokenized']:
    for word in list1:
        if word in palavras_vazias:
            list1.remove(word)
    list1 = " ".join(list1)
    all.append(list1)
dftoken2 = pd.DataFrame({'Tokens': all})
data['cleaned'] = dftoken2
data[['Ementa', 'cleaned']]

data = data.dropna()

import spacy
nlp = spacy.load("pt_core_news_sm")

import lemminflect
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
ps = PorterStemmer()
lem = WordNetLemmatizer()

# Stemming e lemmatização das palavras
words = word_tokenize(data['cleaned'].iloc[0])
for w in words:
    print(w, " : ", ps.stem(w))

lemminf = []
words = []
doc = nlp(data['Ementa'].iloc[0])
for token in doc:
    print(token, " : ", token._.lemma())

stemmedtoken = []
for i in range(0, len(data)):
    emptylist = []
    emptyliststem = []
    words = word_tokenize(data['cleaned'].iloc[i])
    for w in words:
        emptyliststem.append(ps.stem(w))
    stemmedtoken.append(emptyliststem)
data['stemmed_cleaned'] = stemmedtoken
data[['Ementa', 'cleaned', 'stemmed_cleaned']]

data['stemmed_cleaned'] = data['stemmed_cleaned'].apply(' '.join)
data

lemminf = []
for i in range(len(data)):
    words = []
    doc = nlp(data['Ementa'].iloc[i])
    for token in doc:
        if str(token) not in palavras_vazias:
            words.append(token._.lemma())
    lemminf.append((" ".join(words)))

data

dflemm = pd.DataFrame(lemminf)
data['Lemminf'] = dflemm
data

# Vetorização
data = data.dropna()
from sklearn.feature_extraction.text import TfidfVectorizer
corpus = data['Lemminf']
vectorizer = TfidfVectorizer()
E = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names_out()

corpus = data['stemmed_cleaned']
vectorizer = TfidfVectorizer()
Z = vectorizer.fit_transform(corpus)
feature_names_Z = vectorizer.get_feature_names_out()
Z

df_tfidfvectZ = pd.DataFrame(data=Z.toarray(), columns=feature_names_Z)
df_tfidfvectZ

data

# Agrupamento Usando Kmeans

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=23, init='k-means++', random_state=42)
y_predict_Z = kmeans.fit_predict(Z)

data['y_predict_stemm'] = pd.DataFrame(y_predict_Z)

kmeans = KMeans(n_clusters=12, init='k-means++', random_state=42)
y_predict_E = kmeans.fit_predict(E)
data['y_predict_inflemm'] = pd.DataFrame(y_predict_E)

data['count'] = 1

import plotly.express as px

fig = px.bar(data, x="y_predict_stemm", y="count", color="Label", title="Long-Form Input")
fig.show()

from collections import Counter

# Agrupe os dados por cluster
cluster_groups = data.groupby('y_predict_stemm')

# Conte a frequência de cada palavra em cada cluster
word_counts = {}
for name, group in cluster_groups:
    words = ' '.join(group['Lemminf']).split()
    word_counts[name] = Counter(words)

# Obtenha as 23 principais palavras para cada cluster
top_words = {}
for cluster, counter in word_counts.items():
    top_words[cluster] = [word for word, count in counter.most_common(23)]
for cluster, words in top_words.items():
    print(f"Cluster {cluster}: {', '.join(words)}")

fig = px.bar(data, x="y_predict_inflemm", y="count", color="Label", title="Long-Form Input")
fig.show()

from collections import Counter

# Agrupe os dados por cluster
cluster_groups = data.groupby('y_predict_inflemm')

# Conte a frequência de cada palavra em cada cluster
word_counts = {}
for name, group in cluster_groups:
    words = ' '.join(group['Lemminf']).split()
    word_counts[name] = Counter(words)

# Obtenha as 23 principais palavras para cada cluster
top_words = {}
for cluster, counter in word_counts.items():
    top_words[cluster] = [word for word, count in counter.most_common(23)]
for cluster, words in top_words.items():
    print(f"Cluster {cluster}: {', '.join(words)}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Crie dados de amostra
distortions = []
K = range(1, 23)
for k in K:
    kmean = KMeans(n_clusters=k, random_state=7)
    kmean.fit(E)
    distortions.append(kmean.inertia_)

plt.figure(figsize=(23, 5))
plt.plot(K, distortions, '-', color='g')
plt.xlabel('k values')
plt.ylabel('Distortion')
plt.title('O Método do Cotovelo mostrando o k ótimo')
plt.show()

from sklearn.metrics import silhouette_score
from yellowbrick.cluster import KElbowVisualizer

K = range(2, 20)
silhouette = []
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(E)
    preds = kmeanModel.predict(E)
    silhouette.append(silhouette_score(E, preds))

plt.figure(figsize=(20, 5))
plt.plot(K, silhouette, '-', color='g')
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score de cada valor de k')
plt.show()

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
PCA = pca.fit(E.toarray())
X_pca = pca.transform(E.toarray())

kmeans = KMeans()
visualizer = KElbowVisualizer(kmeans, k=(1, 20), size=(1080, 500))

visualizer.fit(X_pca)
visualizer.show()

kmeanModel = KMeans(n_clusters=3)
kmeanModel.fit(X_pca)
pred_labels = kmeanModel.labels_;
pred_centers = kmeanModel.cluster_centers_
df_centers = pd.DataFrame(pred_centers, columns=['x', 'y'])
df_centers.head(1)

dfcl = pd.DataFrame(columns=['x', 'y', 'label'])
dfcl['x'] = X_pca[:, 0]
dfcl['y'] = X_pca[:, 1]
dfcl['label'] = kmeanModel.labels_
dfcl.head(1)

import plotly.express as px
fig = px.scatter(dfcl, x="x", y="y", color="label")
fig.show()

"""#RandomForestClassifier"""

# Classificação
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(E, data['Label'], test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=1000, max_depth=50, random_state=42)

# treine o classificador nos dados de treinamento vetorizados
rf.fit(X_train, y_train)
from sklearn.metrics import accuracy_score
# faça previsões nos dados de teste vetorizados
y_pred = rf.predict(X_test)
# calcule a pontuação de precisão
accuracy = accuracy_score(y_test, y_pred)
print("Precisão:", accuracy)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC
svm_model = SVC(kernel='linear', C=1)
svm_model.fit(X_train, y_train)

# Avalie o desempenho do modelo nos dados de teste
y_pred_1 = svm_model.predict(X_test)
from sklearn.metrics import accuracy_score

# calcule a pontuação de precisão
accuracy = accuracy_score(y_test, y_pred_1)

print("Precisão:", accuracy)

from sklearn.feature_extraction.text import TfidfVectorizer
import scipy.cluster.hierarchy as shc


vectorizer = TfidfVectorizer()
X = E
feature_names
distance_matrix = shc.distance.pdist(X.toarray())
dendrogram = shc.dendrogram(shc.linkage(distance_matrix, method='ward', metric='euclidean'))
plt.figure(figsize=(100, 2300))

plt.show()

"""#Hierarchical Clusterin
Este método agrupa os dados em uma hierarquia de clusters. Existem duas abordagens principais para a hierarquia de clustering: aglomerativa (ou bottom-up) e divisiva (ou top-down).
"""

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Hierarchical Clustering
Z = linkage(df_tfidfvectZ, 'ward')

# Dendrograma
plt.figure(figsize=(25, 10))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(
    Z,
    leaf_rotation=90.,  # Rotaciona os rótulos dos eixos x
    leaf_font_size=8.,  # Tamanho da fonte dos rótulos dos eixos x
)
plt.show()

"""#Spectral Clustering
Este método usa a matriz de afinidade dos dados para mapear os pontos de dados em um espaço de baixa dimensão e, em seguida, agrupa os pontos de dados neste espaço de baixa dimensão.
"""

# Aplicação do Spectral Clustering
from sklearn.cluster import SpectralClustering

spectral = SpectralClustering(n_clusters=23, random_state=42)
y_predict_spectral = spectral.fit_predict(E)
data['y_predict_spectral'] = pd.DataFrame(y_predict_spectral)

fig = px.bar(data, x="y_predict_spectral", y="count", color="Label", title="Long-Form Input")
fig.show()

# Agrupe os dados por cluster
cluster_groups_spectral = data.groupby('y_predict_spectral')

# Conte a frequência de cada palavra em cada cluster
word_counts_spectral = {}
for name, group in cluster_groups_spectral:
    words = ' '.join(group['Lemminf']).split()
    word_counts_spectral[name] = Counter(words)

# Obtenha as 15 principais palavras para cada cluster
top_words_spectral = {}
for cluster, counter in word_counts_spectral.items():
    top_words_spectral[cluster] = [word for word, count in counter.most_common(15)]
for cluster, words in top_words_spectral.items():
    print(f"Cluster {cluster}: {', '.join(words)}")

"""#Mean Shift
Este é um algoritmo de clusterização baseado em densidade que procura centróides de clusters movendo-se para regiões de maior densidade de pontos de dados.
"""

# Teste Mean Shift
from sklearn.cluster import MeanShift


meanshift = MeanShift()
meanshift.fit(E.toarray())
labels = meanshift.labels_
data['y_predict_meanshift'] = labels

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Mean Shift Clustering')
plt.show()

"""#Hierarchical Clustering
O Hierarchical Clustering é um algoritmo de agrupamento que constrói uma hierarquia de clusters. Ele começa com cada ponto de dados como um cluster e, em seguida, mescla os clusters mais próximos até que todos os pontos de dados estejam em um único cluster
"""

from sklearn.cluster import AgglomerativeClustering

n_clusters = 23
hierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters)
y_predict_Z = hierarchical_clustering.fit_predict(Z)
data['y_predict_stemm'] = pd.DataFrame(y_predict_Z)
hierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters)
y_predict_E = hierarchical_clustering.fit_predict(E.toarray())
data['y_predict_inflemm'] = pd.DataFrame(y_predict_E)
data['count'] = 1

import plotly.express as px

fig = px.bar(data, x="y_predict_stemm", y="count", color="Label", title="Long-Form Input")
fig.show()

from collections import Counter

cluster_groups = data.groupby('y_predict_stemm')

word_counts = {}
for name, group in cluster_groups:
    words = ' '.join(group['Lemminf']).split()
    word_counts[name] = Counter(words)

top_words = {}
for cluster, counter in word_counts.items():
    top_words[cluster] = [word for word, count in counter.most_common(15)]
for cluster, words in top_words.items():
    print(f"Cluster {cluster}: {', '.join(words)}")

fig = px.bar(data, x="y_predict_inflemm", y="count", color="Label", title="Long-Form Input")
fig.show()

from collections import Counter

# Agrupe os dados por cluster
cluster_groups = data.groupby('y_predict_inflemm')

# Conte a frequência de cada palavra em cada cluster
word_counts = {}
for name, group in cluster_groups:
    words = ' '.join(group['Lemminf']).split()
    word_counts[name] = Counter(words)

# Obtenha as 15 principais palavras para cada cluster
top_words = {}
for cluster, counter in word_counts.items():
    top_words[cluster] = [word for word, count in counter.most_common(15)]
for cluster, words in top_words.items():
    print(f"Cluster {cluster}: {', '.join(words)}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Crie dados de amostra
distortions = []
K = range(1, 20)
for k in K:
    kmean = KMeans(n_clusters=k, random_state=7)
    kmean.fit(E)
    distortions.append(kmean.inertia_)

plt.figure(figsize=(20, 5))
plt.plot(K, distortions, '-', color='g')
plt.xlabel('k values')
plt.ylabel('Distortion')
plt.title('O Método mostrando o k ótimo')
plt.show()

from sklearn.metrics import silhouette_score
from yellowbrick.cluster import KElbowVisualizer

K = range(2, 20)
silhouette = []
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(E)
    preds = kmeanModel.predict(E)
    silhouette.append(silhouette_score(E, preds))

plt.figure(figsize=(20, 5))
plt.plot(K, silhouette, '-', color='g')
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score de cada valor de k')
plt.show()

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
PCA = pca.fit(E.toarray())
X_pca = pca.transform(E.toarray())

kmeans = KMeans()
visualizer = KElbowVisualizer(kmeans, k=(1, 20), size=(1080, 500))

visualizer.fit(X_pca)
visualizer.show()

kmeanModel = KMeans(n_clusters=3)
kmeanModel.fit(X_pca)
pred_labels = kmeanModel.labels_;
pred_centers = kmeanModel.cluster_centers_
df_centers = pd.DataFrame(pred_centers, columns=['x', 'y'])
df_centers.head(1)

dfcl = pd.DataFrame(columns=['x', 'y', 'label'])
dfcl['x'] = X_pca[:, 0]
dfcl['y'] = X_pca[:, 1]
dfcl['label'] = kmeanModel.labels_
dfcl.head(1)

import plotly.express as px
fig = px.scatter(dfcl, x="x", y="y", color="label")
fig.show()

# Classificação

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(E, data['Label'], test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# treine o classificador nos dados de treinamento vetorizados
rf.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

# faça previsões nos dados de teste vetorizados
y_pred = rf.predict(X_test)

# calcule a pontuação de precisão
accuracy = accuracy_score(y_test, y_pred)

print("Precisão:", accuracy)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC
svm_model = SVC(kernel='linear', C=1)
svm_model.fit(X_train, y_train)

# Avalie o desempenho do modelo nos dados de teste
y_pred_1 = svm_model.predict(X_test)
from sklearn.metrics import accuracy_score


# calcule a pontuação de precisão
accuracy = accuracy_score(y_test, y_pred_1)

print("Precisão:", accuracy)

# Defina os dados da árvore
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Hierarchical Clustering
Z = linkage(df_tfidfvectZ, 'ward')

# Dendrograma
plt.figure(figsize=(25, 10))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(
    Z,
    leaf_rotation=90.,  # Rotaciona os rótulos dos eixos x
    leaf_font_size=8.,  # Tamanho da fonte dos rótulos dos eixos x
)
plt.show()